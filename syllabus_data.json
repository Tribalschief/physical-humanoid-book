{
    "intro": {
        "title": "Introduction to Physical AI",
        "content": "# Introduction to Physical AI & Humanoid Robotics\n\n## Course Overview\nThe future of work will be a partnership between people, intelligent agents, and robots. This course bridges the gap between digital AI and the physical world.\n\n### Why Physical AI Matters\nHumanoid robots are poised to excel in our human-centered world because they share our physical form and can be trained with abundant data from interacting in human environments. This represents a significant transition from AI models confined to digital environments to embodied intelligence that operates in physical space.\n\n### Embodied Intelligence\nEmbodied intelligence refers to intelligent agents that have a physical body and interact with the physical world. Unlike Chatbots which only process text, Physical AI must process gravity, friction, collisions, and visual depth.\n\n## Hardware Requirements\n- **Workstation**: NVIDIA RTX 4070 Ti (12GB) or higher, Intel Core i7 13th Gen, 64GB RAM, Ubuntu 22.04.\n- **Edge Computing**: NVIDIA Jetson Orin Nano (8GB) for the robot brain.\n- **Sensors**: Intel RealSense D435i (Depth Camera + IMU), ReSpeaker Mic Array.\n- **Robot**: Unitree Go2 (Quadruped) or G1 (Humanoid) for Sim-to-Real transfer.\n"
    },
    "modules": [
        {
            "id": "module-1",
            "title": "Module 1: The Robotic Nervous System (ROS 2)",
            "pages": [
                {
                    "slug": "ros2-fundamentals",
                    "title": "ROS 2 Fundamentals",
                    "content": "# ROS 2 Fundamentals\n\nROS 2 (Robot Operating System) is the middleware that acts as the nervous system for our robots. It adopts a Data Distribution Service (DDS) standard for real-time communication.\n\n## Core Concepts\n\n### 1. Nodes\nA **Node** is a single process that performs a specific computation. In a humanoid, you might have one node for the camera driver, one for face detection, and one for motor control.\n\n### 2. Topics\nNodes communicate via **Topics** (Publish/Subscribe). \n- The Camera Node *publishes* images to the `/camera/image_raw` topic.\n- The Face Detection Node *subscribes* to that topic.\n\n### 3. Services\nServices use a Request/Response model. Use them for actions that terminate quickly, like `reset_simulation` or `calibrate_sensors`.\n\n## Practical: Installing ROS 2 Humble\n```bash\nsudo apt update && sudo apt install locales\nsudo locale-gen en_US en_US.UTF-8\nsudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8\nexport LANG=en_US.UTF-8\n\nsudo apt install software-properties-common\nsudo add-apt-repository universe\n\nsudo apt update && sudo apt install curl -y\nsudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg\n\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release && echo $UBUNTU_CODENAME) main\" | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null\n\nsudo apt update\nsudo apt install ros-humble-desktop\n```"
                },
                {
                    "slug": "python-nodes",
                    "title": "Building ROS 2 Packages",
                    "content": "# Building ROS 2 Packages with Python\n\nWe use `rclpy` to bridge Python AI agents with robotic hardware.\n\n## Workspace Setup\n```bash\nmkdir -p ~/ros2_ws/src\ncd ~/ros2_ws/src\nros2 pkg create --build-type ament_python my_physical_ai_pkg\ncd ~/ros2_ws\ncolcon build\nsource install/setup.bash\n```\n\n## Writing a Controller Node\nThis node listens to sensor data and publishes motor commands.\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\nimport random\n\nclass HumanoidWalker(Node):\n    def __init__(self):\n        super().__init__('humanoid_walker')\n        self.pub = self.create_publisher(Twist, '/cmd_vel', 10)\n        self.timer = self.create_timer(0.1, self.move)\n        self.get_logger().info(\"Walker Node Started\")\n\n    def move(self):\n        msg = Twist()\n        # Simulate simple walking logic\n        msg.linear.x = 0.5  # Walk forward\n        msg.angular.z = random.uniform(-0.1, 0.1) # Balance correction\n        self.pub.publish(msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = HumanoidWalker()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n```\n"
                },
                {
                    "slug": "urdf-humanoid",
                    "title": "URDF and Kinematics",
                    "content": "# Unified Robot Description Format (URDF)\n\nURDF files describe the physical properties of your robot using XML.\n\n## Humanoid Structure\nA humanoid URDF includes links (bones) and joints (motors).\n\n```xml\n<robot name=\"simple_humanoid\">\n  <!-- Torso Link -->\n  <link name=\"torso\">\n    <visual>\n      <geometry>\n        <box size=\"0.3 0.5 0.2\"/>\n      </geometry>\n    </visual>\n  </link>\n\n  <!-- Neck Joint -->\n  <joint name=\"neck_joint\" type=\"revolute\">\n    <parent link=\"torso\"/>\n    <child link=\"head\"/>\n    <origin xyz=\"0 0.3 0\"/>\n    <axis xyz=\"0 0 1\"/>\n    <limit lower=\"-1.57\" upper=\"1.57\" effort=\"10\" velocity=\"1.0\"/>\n  </joint>\n</robot>\n```\n\n## Forward vs Inverse Kinematics\n- **Forward Kinematics (FK)**: Given joint angles (e.g., Elbow 90Â°), where is the hand?\n- **Inverse Kinematics (IK)**: Check \"MoveIt 2\" module. Given \"Hand at (x,y,z)\", what are the joint angles?\n"
                }
            ]
        },
        {
            "id": "module-2",
            "title": "Module 2: The Digital Twin (Gazebo & Unity)",
            "pages": [
                {
                    "slug": "gazebo-environment",
                    "title": "Gazebo Simulation",
                    "content": "# Robot Simulation with Gazebo\n\nGazebo is the standard for physics-based robot simulation. It handles Rigid Body Dynamics (Gravity, Inertia), Battery consumption, and Sensor noise generation.\n\n## Launching Gazebo\n```bash\nsudo apt install ros-humble-gazebo-ros-pkgs\nros2 launch gazebo_ros gazebo.launch.py\n```\n\n## Creating Worlds (SDF)\nWe use SDF (Simulation Description Format) to define the environment.\n```xml\n<sdf version=\"1.6\">\n  <world name=\"default\">\n    <include>\n      <uri>model://sun</uri>\n    </include>\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n    <!-- Add obstacles here -->\n  </world>\n</sdf>\n```\n"
                },
                {
                    "slug": "sensor-simulation",
                    "title": "Simulating Sensors",
                    "content": "# Simulating Physical Sensors\n\nTo bridge the Sim-to-Real gap, our virtual robot must have the same senses as the physical one.\n\n### LiDAR Simulation\nGazebo uses plugins to simulate sensors. Here is a Ray Sensor logic:\n```xml\n<sensor type=\"ray\" name=\"head_hokuyo_sensor\">\n  <pose>0 0 0 0 0 0</pose>\n  <visualize>true</visualize>\n  <update_rate>40</update_rate>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>720</samples>\n        <resolution>1</resolution>\n        <min_angle>-1.570796</min_angle>\n        <max_angle>1.570796</max_angle>\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.10</min>\n      <max>30.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <plugin name=\"gazebo_ros_head_hokuyo_controller\" filename=\"libgazebo_ros_ray_sensor.so\">\n    <ros>\n      <namespace>/humanoid</namespace>\n      <remapping>~/out:=scan</remapping>\n    </ros>\n    <output_type>sensor_msgs/LaserScan</output_type>\n  </plugin>\n</sensor>\n```\n"
                },
                {
                    "slug": "unity-visualization",
                    "title": "Unity for Visualization",
                    "content": "# Unity and ROS 2\n\nUnity provides High-Fidelity Rendering that Gazebo lacks. This is crucial for Human-Robot Interaction (HRI).\n\n## Setup\n1. Install Unity Hub.\n2. Create a new 3D URP Project.\n3. Import the `ROS-TCP-Connector` package.\n4. In Unity, set the ROS IP to your local machine (127.0.0.1) and Port 10000.\n5. In ROS 2, run the endpoint: `ros2 run ros_tcp_endpoint default_server_endpoint`.\n\nNow you can visualize topics directly in Unity's Scene view."
                }
            ]
        },
        {
            "id": "module-3",
            "title": "Module 3: The AI-Robot Brain (NVIDIA Isaac)",
            "pages": [
                {
                    "slug": "isaac-sim",
                    "title": "NVIDIA Isaac Sim",
                    "content": "# NVIDIA Isaac Platform\n\nNVIDIA Isaac Sim is built on the Omniverse platform and enables photorealistic simulation with RTX efficiency.\n\n## Installing Isaac Sim\n1. Download NVIDIA Omniverse Launcher.\n2. Install \"Isaac Sim\" from the Exchange.\n3. Ensure your drivers are 535+.\n\n## USD Assets\nIsaac Sim uses Universal Scene Description (USD) files. You can drag and drop robots like the Unitree Go2 directly from the Asset Browser.\n\n## Python API (OmniKit)\n```python\nfrom omni.isaac.core import World\nfrom omni.isaac.core.objects import DynamicCuboid\nimport numpy as np\n\nworld = World()\nworld.scene.add_default_ground_plane()\ncube = world.scene.add(\n    DynamicCuboid(\n        prim_path=\"/World/Cube\",\n        name=\"cube\",\n        position=np.array([0, 0, 1.0]),\n        scale=np.array([0.5, 0.5, 0.5])\n    )\n)\nworld.reset()\nfor i in range(500):\n    world.step(render=True)\n```"
                },
                {
                    "slug": "isaac-ros",
                    "title": "Isaac ROS & VSLAM",
                    "content": "# Isaac ROS GEMs\n\nHardware-accelerated ROS 2 packages that run on the Jetson Orin.\n\n## VSLAM (Visual SLAM)\nUsing the stereo cameras to map a room and locate the robot within it, without GPS. \n`isaac_ros_visual_slam` provides a high-performance node for this, utilizing the Elbrus VSLAM library.\n\n## Launching VSLAM\n```bash\nros2 launch isaac_ros_visual_slam isaac_ros_visual_slam.launch.py camera_input:=/camera/stereo/image_raw\n```\n\n## Nvblox\nFor 3D reconstruction/occupancy grids, we use `nvblox` which processes depth maps on the GPU to create a costmap for Nav2 in real-time."
                }
            ]
        },
        {
            "id": "module-4",
            "title": "Module 4: Vision-Language-Action (VLA)",
            "pages": [
                {
                    "slug": "vla-intro",
                    "title": "Vision-Language-Action Models",
                    "content": "# Vision-Language-Action (VLA)\n\nTraditional robotics uses separate modules (Vision -> Planning -> Control). VLA models (like RT-2) merge these into a single Transformer model.\n\n## Tokenizing Action\nThe key breakthrough is treating robot actions as \"words\" (tokens). \n- Input: Image Tokens + Text Tokens (\"Pick up apple\")\n- Output: Text Tokens + Action Tokens (End-Effector Pose Change)\n\n## Open Source VLAs\n- **OpenVLA**: A Llama-3 based model fine-tuned on the BridgeV2 dataset.\n- **Octo**: A transformer-based policy trained on 800k trajectories.\n"
                },
                {
                    "slug": "voice-command",
                    "title": "Voice & Conversion",
                    "content": "# Conversational Robotics Pipeline\n\n## 1. Whisper (Speech to Text)\nRun Whisper on the Jetson Orin using `whisper.cpp` for real-time performance.\n\n## 2. LLM Planner (Agent)\nThe LLM receives the prompt:\n\"You are a robot controller. Available actions: [navigate(loc), pick(obj), place(loc)]. User says: 'Put the cup in the sink'.\"\n\n**Output:**\n1. `navigate(\"table\")`\n2. `pick(\"cup\")`\n3. `navigate(\"sink\")`\n4. `place(\"sink\")`\n"
                },
                {
                    "slug": "capstone-project",
                    "title": "Capstone: The Autonomous Humanoid",
                    "content": "# Capstone Project: Home Service Robot\n\n**Scenario**: Robot starts in Living Room. Needs to tidy up.\n\n## Requirements\n1. **Mapping**: Use SLAMtoolbox to map the apartment in Gazebo/Isaac Sim.\n2. **Navigation**: Use Nav2 to autonomously move between rooms.\n3. **Perception**: Use YOLOv8 to detect \"Toys\" on the floor.\n4. **Manipulation**: Use MoveIt 2 to plan a grasp for the toy and drop it in the \"Toy Box\".\n\n## Grading\n- 80%: Functionality (Does it pick up the toy?)\n- 20%: Code Quality (ROS 2 best practices, Launch files, README)."
                }
            ]
        }
    ]
}